"""
"""

from . import core

import numpy as np

import theano
import theano.tensor as T
from theano.tensor.shared_randomstreams import RandomStreams
from theano.tensor.signal import downsample

from . import FLOATX
# from . import TENSOR_TYPES
# from . import functions
# from .. import urls


# --- Node Implementations ------
class Node(core.Struct):
    """
    Nodes in the graph perform parameter management and micro-math operations.

    Note that a "Node" is merely a base class, and cannot be instantiated.
    Or, empty dictionaries? Which makes more sense...? Loud failure of a class
    that shouldn't be used will certainly prevent misuse...

    MUST implement three property getters:
        self._input_shapes
        self._output_shapes
        self._param_shapes

    Param name defines should come from here, not the args class.
    Move arg classes to separate module, they exist for conveience of creating
    properly formatted dictionaries.
    """

    def __init__(self, inputs, outputs, params, scalars, act_type):
        """
        act_type : str
        inputs : Struct of Ports
        outputs : Struct of Ports
        params : Struct of Parameters
        scalars : Struct of Scalars
        """
        core.Struct.__init__(
            self, act_type=act_type, inputs=inputs, outputs=outputs,
            params=params, scalars=scalars)

        self._numpy_rng = np.random.RandomState()
        self._theano_rng = RandomStreams(self._numpy_rng.randint(2 ** 30))

    # --- Public Properties ---
    @property
    def type(self):
        """writeme"""
        return self.__class__.__name__

    @property
    def activation(self):
        """writeme"""
        return functions.Activations.get(self.act_type)

    def transform(self):
        """writeme"""
        raise NotImplementedError("Subclass me!")

    # This should be implemented by whatever is up the stack...
    # -------
    # def copy(self, new_name):
    #     """writeme"""
    #     args = dict(**self)
    #     args.update(name=new_name)
    #     node = self.__class__(**args)
    #     for k in self.params:
    #         node.params[k].value = p.value
    #     return node


    # def validate_inputs(self, inputs):
    #     """Determine if the inputs dictionary contains the necessary keys."""
    #     # TODO(ejhumphrey): Look into this method. It has failed opaquely
    #     # in the past, and I'm not sure why.
    #     return not False in [url in inputs for url in self.inputs]

    # def filter_inputs(self, inputs):
    #     """Extract the relevant input items, iff all are present."""
    #     assert self.validate_inputs(inputs)
    #     return dict([(k, inputs.pop(k)) for k in self.inputs])


class Affine(Node):
    """
    Affine Transform Layer
      (i.e., a fully-connected non-linear projection)

    """
    def __init__(self, input_shape, output_shape, act_type,
                 enable_dropout=False):
        n_in = int(np.prod(input_shape))
        n_out = int(np.prod(output_shape))
        # Note that Inputs/Outputs/Params are named here.
        inputs = core.Struct(x_in=core.Port(input_shape))
        outputs = core.Struct(z_out=core.Port(output_shape))
        params = core.Struct(weights=core.Parameter([n_in, n_out]),
                             bias=core.Parameter([n_out, ]))
        scalars = core.Struct()
        if enable_dropout:
            scalars.update(dropout=core.Scalar())

        Node.__init__(self, inputs=inputs, outputs=outputs, params=params,
                      scalars=scalars, act_type=act_type)


    def transform(self, inputs):
        """
        will fix input tensors to be matrices as the following:
        (N x d0 x d1 x ... dn) -> (N x prod(d_(0:n)))

        Parameters
        ----------
        inputs: dict
            Must contain all known data inputs to this node, keyed by full
            URLs. Will fail loudly otherwise.

        Returns
        -------
        outputs: dict
            Will contain all outputs generated by this node, keyed by full
            name. Note that the symbolic outputs will take this full name
            internal to each object.
        """
        raise NotImplementedError("come back to this")
        assert self.validate_inputs(inputs)
        # Since there's only the one, just take the single item.
        x_in = inputs.get(self.inputs[0])
        weights = self._params[self._WEIGHTS]
        bias = self._params[self._BIAS].dimshuffle('x', 0)

        # TODO(ejhumphrey): This isn't very stable, is it.
        x_in = T.flatten(x_in, outdim=2)
        z_out = self.activation(T.dot(x_in, weights) + bias)

        output_shape = self._output_shapes[self._OUTPUT]
        selector = self.theano_rng.binomial(size=output_shape,
                                            p=1.0 - self.dropout,
                                            dtype=FLOATX)
        z_out *= selector.dimshuffle('x', 0) * (self.dropout + 0.5)
        z_out.name = self.outputs[0]
        return {z_out.name: z_out}


class Conv3D(Node):
    """ (>^.^<) """

    def __init__(self,
                 input_shape,
                 weight_shape,
                 pool_shape=(1, 1),
                 downsample_shape=(1, 1),
                 act_type='relu',
                 border_mode='valid',
                 enable_dropout=False):
        """
        Parameters
        ----------
        input_shape : tuple
            Shape of the input data, as (in_maps, in_dim0, in_dim1).
        weight_shape : tuple
            Shape for all kernels, as (num_kernels, w_dim0, w_dim1).
        pool_shape : tuple
            2D tuple to pool over each feature map, as (p_dim0, p_dim1).
        downsample_shape : tuple
            2D tuple for downsampling each feature map, as (p_dim0, p_dim1).
        act_type : str
            Name of the activation function to use.
        border_mode : str
            Convolution method for dealing with the edge of a feature map.

        """
        # Make sure the weight_shape argument is formatted properly.
        w = list(weight_shape)
        if len(w) == 3:
            w.insert(1, input_shape[0])
        elif len(w) == 4:
            assert w[1] == input_shape[0], \
                "weight_shape[1] must align with input_shape[0]: " \
                "%d!=%d." % (w[1], input_shape[0])
        else:
            raise ValueError("'weight_shape' must be length 3 or 4.")
        weight_shape = tuple(w)

        d0_in, d1_in = input_shape[-2:]
        if border_mode == 'valid':
            d0_out = int(d0_in - weight_shape[-2] + 1)
            d0_out /= pool_shape[0]
            d1_out = int(d1_in - weight_shape[-1] + 1)
            d1_out /= pool_shape[1]
        elif border_mode == 'same':
            d0_out, d1_out = d0_in, d1_in
        elif border_mode == 'full':
            raise NotImplementedError("Haven't implemented 'full' shape yet.")

        output_shape = (weight_shape[0], d0_out, d1_out)

        inputs = core.Struct(x_in=core.Port(input_shape))
        outputs = core.Struct(z_out=core.Port(output_shape))
        params = core.Struct(weights=core.Parameter(weight_shape),
                             bias=core.Parameter(weight_shape[:1]))
        scalars = core.Struct()
        if enable_dropout:
            scalars.update(dropout=core.Scalar())

        Node.__init__(self, inputs=inputs, outputs=outputs, params=params,
                      scalars=scalars, act_type=act_type)

        self.pool_shape = pool_shape
        self.downsample_shape = downsample_shape
        self.border_mode = border_mode

        fan_in = np.prod(weight_shape[1:])
        weight_values = self._numpy_rng.normal(
            loc=0.0, scale=np.sqrt(3. / fan_in), size=weight_shape)

        if act_type == 'sigmoid':
            weight_values *= 4

        self.params.weights.value = weight_values

    def transform(self, inputs):
        """writeme."""
        raise NotImplementedError("come back to this")
        self.validate_inputs(inputs)
        x_in = inputs.get(self.inputs[0])
        weights = self._params[self._WEIGHTS]
        bias = self._params[self._BIAS].dimshuffle('x', 0, 'x', 'x')

        z_out = T.nnet.conv.conv2d(
            input=x_in,
            filters=weights,
            filter_shape=self._weight_shape,
            border_mode=self._border_mode)

        selector = self.theano_rng.binomial(
            size=self._output_shapes[self._OUTPUT][:1],
            p=1.0 - self.dropout,
            dtype=FLOATX)

        z_out = self.activation(z_out + bias)
        z_out *= selector.dimshuffle('x', 0, 'x', 'x') * (self.dropout + 0.5)
        z_out = downsample.max_pool_2d(
            z_out, self._pool_shape, ignore_border=False)
        z_out.name = self.outputs[0]
        return {z_out.name: z_out}


class Conv2D(Node):
    """ . """

    def __init__(self, layer_args):
        """
        layer_args : ConvArgs

        """
        raise NotImplementedError("come back to this")
        Node.__init__(self, layer_args)

        # Create all the weight values at once
        weight_shape = self.param_shapes.get("weights")
        fan_in = np.prod(weight_shape[1:])
        weights = self.numpy_rng.normal(loc=0.0,
                                        scale=np.sqrt(3. / fan_in),
                                        size=weight_shape)

        if self.get("activation") == 'sigmoid':
            weights *= 4

        bias = np.zeros(weight_shape[0])
        self.param_values = {self.own('weights'): weights,
                             self.own('bias'): bias, }

    def transform(self, x_in):
        """writeme"""
        raise NotImplementedError("come back to this")
        W = self._theta['weights']
        b = self._theta['bias']
        weight_shape = self.param_shapes.get("weights")
        z_out = T.nnet.conv.conv2d(input=x_in,
                                   filters=W,
                                   filter_shape=weight_shape,
                                   border_mode=self.get("border_mode"))

        selector = self.theano_rng.binomial(size=self.output_shape[:1],
                                            p=1.0 - self.dropout,
                                            dtype=FLOATX)

        z_out = self.activation(z_out + b.dimshuffle('x', 0, 'x', 'x'))
        z_out *= selector.dimshuffle('x', 0, 'x', 'x') * (self.dropout + 0.5)
        return downsample.max_pool_2d(
            z_out, self.get("pool_shape"), ignore_border=False)


class Softmax(Affine):
    """
    """

    def __init__(self, input_shape, output_shape, act_type):
        """
        """
        Affine.__init__(
            self, input_shape, output_shape, act_type, enable_dropout=False)

    def transform(self, inputs):
        """
        will fix input tensors to be matrices as the following:
        (N x d0 x d1 x ... dn) -> (N x prod(d_(0:n)))
        """
        raise NotImplementedError("come back to this")
        assert self.validate_inputs(inputs)
        # Since there's only the one, just take the single item.
        x_in = inputs.get(self.inputs[0])
        weights = self._params[self._WEIGHTS]
        bias = self._params[self._BIAS].dimshuffle('x', 0)

        # TODO(ejhumphrey): This isn't very stable, is it.
        # Use the given output shape to handle a multi-dim softmax output.
        x_in = T.flatten(x_in, outdim=2)
        z_out = T.nnet.softmax(self.activation(T.dot(x_in, weights) + bias))
        z_out.name = self.outputs[0]
        return {z_out.name: z_out}


class LpDistance(Node):
    """
    Distance Node

    """
    _INPUT_A = 'input-A'
    _INPUT_B = 'input-B'
    _OUTPUT = 'output'
    _P = "p"

    def __init__(self, name, p=2.0):
        """

        """
        raise NotImplementedError("come back to this")
        self.update(dict(p=p))
        Node.__init__(self, name, act_type='linear')
        self._scalars.clear()

    # --- Over-ridden private properties ---
    @property
    def _input_shapes(self):
        """
        Returns
        -------
        shapes : dict
        """
        return {self._INPUT_A: (), self._INPUT_B: ()}

    @property
    def _output_shapes(self):
        """
        Returns
        -------
        shapes : dict
        """
        return {self._OUTPUT: ()}

    @property
    def _param_shapes(self):
        return {}

    def transform(self, inputs):
        """
        Parameters
        ----------
        inputs: dict
            Must contain all known data inputs to this node, keyed by full
            URLs. Will fail loudly otherwise.

        Returns
        -------
        outputs: dict
            Will contain all outputs generated by this node, keyed by full
            name. Note that the symbolic outputs will take this full name
            internal to each object.
        """
        assert self.validate_inputs(inputs)

        xA = T.flatten(inputs.get(self._own(self._INPUT_A)), outdim=2)
        xB = T.flatten(inputs.get(self._own(self._INPUT_B)), outdim=2)
        p = self.get(self._P)
        z_out = T.pow(T.pow(T.abs_(xA - xB), p).sum(axis=1), 1.0 / p)
        z_out.name = self.outputs[0]
        return {z_out.name: z_out}
